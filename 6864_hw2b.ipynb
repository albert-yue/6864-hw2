{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "6864-hw2b.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/albert-yue/6864-hw2/blob/master/6864_hw2b.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FU7xWiY6TyWS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%bash\n",
        "!(stat -t /usr/local/lib/*/dist-packages/google/colab > /dev/null 2>&1) && exit\n",
        "rm -rf 6864-hw2b"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5AyMA9rK1Rhf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "os.makedirs(\"6864-hw2b\", exist_ok=True)\n",
        "import sys\n",
        "sys.path.append(\"/content/6864-hw2b\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BL1IfnRdPdsl",
        "colab_type": "code",
        "outputId": "fe0f2682-4b60-4b88-9964-19acb119448a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "!pip install sacrebleu"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: sacrebleu in /usr/local/lib/python3.6/dist-packages (1.4.4)\n",
            "Requirement already satisfied: typing in /usr/local/lib/python3.6/dist-packages (from sacrebleu) (3.6.6)\n",
            "Requirement already satisfied: portalocker in /usr/local/lib/python3.6/dist-packages (from sacrebleu) (1.6.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5fOArV2r9Piz",
        "colab_type": "text"
      },
      "source": [
        "# **Part 3: Sequence-to-Sequence Model**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mA9JfWiK9eoL",
        "colab_type": "text"
      },
      "source": [
        "In this lab, you will explore RNN-based sequence-to-sequence (seq2seq) models to perform machine translation (MT). We will use a Vietnamese-English dataset from IWSLT'15. The task is to translate a Vietnamese sentence into English.\n",
        "\n",
        "The lab is divided into two parts. The first part is to implement a vanilla seq2seq architecture without attention. In the second part you will implement your favorite attention mechanism (doesn't have to come from lecture) and add it to your vanilla seq2seq model. We will provide the training and testing scripts (trust me, the decoding/testing is actually the hardest part :P), so you will mainly just have to focus on implementing the models (I say *mainly* because you still might need to modify the testing script, depending on which attention method you use and how you implement it).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zDJjmvZfHV_l",
        "colab_type": "text"
      },
      "source": [
        "## **Section 1: Data Preprocessing**\n",
        "\n",
        "No need to write any code in this section. But you are encouraged to test with this part to understand the data.\n",
        "\n",
        "First, we download the dataset and place it under current directory."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "02RioHPryvOz",
        "colab_type": "code",
        "outputId": "de63d2b4-3135-4861-a2f9-2ade7f087af1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "source": [
        "# Download data\n",
        "!wget -nv -O /content/6864-hw2b/train.en https://nlp.stanford.edu/projects/nmt/data/iwslt15.en-vi/train.en\n",
        "!wget -nv -O /content/6864-hw2b/train.vi https://nlp.stanford.edu/projects/nmt/data/iwslt15.en-vi/train.vi\n",
        "!wget -nv -O /content/6864-hw2b/tst2013.en https://nlp.stanford.edu/projects/nmt/data/iwslt15.en-vi/tst2013.en\n",
        "!wget -nv -O /content/6864-hw2b/tst2013.vi https://nlp.stanford.edu/projects/nmt/data/iwslt15.en-vi/tst2013.vi\n",
        "!wget -nv -O /content/6864-hw2b/vocab.en https://nlp.stanford.edu/projects/nmt/data/iwslt15.en-vi/vocab.en\n",
        "!wget -nv -O /content/6864-hw2b/vocab.vi https://nlp.stanford.edu/projects/nmt/data/iwslt15.en-vi/vocab.vi"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-03-24 03:55:49 URL:https://nlp.stanford.edu/projects/nmt/data/iwslt15.en-vi/train.en [13603614/13603614] -> \"/content/6864-hw2b/train.en\" [1]\n",
            "2020-03-24 03:55:59 URL:https://nlp.stanford.edu/projects/nmt/data/iwslt15.en-vi/train.vi [18074646/18074646] -> \"/content/6864-hw2b/train.vi\" [1]\n",
            "2020-03-24 03:56:00 URL:https://nlp.stanford.edu/projects/nmt/data/iwslt15.en-vi/tst2013.en [132264/132264] -> \"/content/6864-hw2b/tst2013.en\" [1]\n",
            "2020-03-24 03:56:01 URL:https://nlp.stanford.edu/projects/nmt/data/iwslt15.en-vi/tst2013.vi [183855/183855] -> \"/content/6864-hw2b/tst2013.vi\" [1]\n",
            "2020-03-24 03:56:02 URL:https://nlp.stanford.edu/projects/nmt/data/iwslt15.en-vi/vocab.en [139741/139741] -> \"/content/6864-hw2b/vocab.en\" [1]\n",
            "2020-03-24 03:56:04 URL:https://nlp.stanford.edu/projects/nmt/data/iwslt15.en-vi/vocab.vi [46767/46767] -> \"/content/6864-hw2b/vocab.vi\" [1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ogFESHAf-6MY",
        "colab_type": "text"
      },
      "source": [
        "Next, we do some simple data preprocessing and show some data statistics."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OfkQGqV30hgC",
        "colab_type": "code",
        "outputId": "c5596f9f-83e9-4cdb-d6cc-d96809eba23b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 455
        }
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "def read_sentence_file(filename):\n",
        "  sentences_list = []\n",
        "  with open(filename, \"r\") as f:\n",
        "    for line in f:\n",
        "      sentences_list.append(line.strip().split())\n",
        "  return sentences_list\n",
        "\n",
        "def read_vocab_file(filename):\n",
        "  with open(filename, \"r\") as f:\n",
        "    return [line.strip() for line in f]\n",
        "\n",
        "\n",
        "src_vocab_set = read_vocab_file(os.path.join(\"/content/6864-hw2b\", \"vocab.vi\"))\n",
        "trg_vocab_set = read_vocab_file(os.path.join(\"/content/6864-hw2b\", \"vocab.en\"))\n",
        "\n",
        "train_src_sentences_list = read_sentence_file(os.path.join(\"/content/6864-hw2b\",\n",
        "                                                           \"train.vi\"))\n",
        "train_trg_sentences_list = read_sentence_file(os.path.join(\"/content/6864-hw2b\",\n",
        "                                                           \"train.en\"))\n",
        "assert len(train_src_sentences_list) == len(train_trg_sentences_list)\n",
        "\n",
        "test_src_sentences_list = read_sentence_file(os.path.join(\"/content/6864-hw2b\",\n",
        "                                                          \"tst2013.vi\"))\n",
        "test_trg_sentences_list = read_sentence_file(os.path.join(\"/content/6864-hw2b\",\n",
        "                                                          \"tst2013.en\"))\n",
        "assert len(test_src_sentences_list) == len(test_trg_sentences_list)\n",
        "\n",
        "\n",
        "MAX_SENT_LENGTH = 48\n",
        "MAX_SENT_LENGTH_PLUS_SOS_EOS = 50\n",
        "\n",
        "# We only keep sentences that do not exceed 48 words, so that later when we\n",
        "# add <s> and </s> to a sentence it still won't exceed 50 words.\n",
        "def filter_data(src_sentences_list, trg_sentences_list, max_len):\n",
        "  new_src_sentences_list, new_trg_sentences_list = [], []\n",
        "  for src_sent, trg_sent in zip(src_sentences_list, trg_sentences_list):\n",
        "    if (len(src_sent) <= max_len and len(trg_sent) <= max_len\n",
        "        and len(src_sent) > 0 and len(trg_sent)) > 0:\n",
        "      new_src_sentences_list.append(src_sent)\n",
        "      new_trg_sentences_list.append(trg_sent)\n",
        "  return new_src_sentences_list, new_trg_sentences_list\n",
        "\n",
        "train_src_sentences_list, train_trg_sentences_list = filter_data(\n",
        "    train_src_sentences_list, train_trg_sentences_list, max_len=MAX_SENT_LENGTH)\n",
        "test_src_sentences_list, test_trg_sentences_list = filter_data(\n",
        "    test_src_sentences_list, test_trg_sentences_list, max_len=MAX_SENT_LENGTH)\n",
        "\n",
        "# We take 10% of training data as validation set.\n",
        "num_val = int(len(train_src_sentences_list) * 0.1)\n",
        "val_src_sentences_list = train_src_sentences_list[:num_val]\n",
        "val_trg_sentences_list = train_trg_sentences_list[:num_val]\n",
        "train_src_sentences_list = train_src_sentences_list[num_val:]\n",
        "train_trg_sentences_list = train_trg_sentences_list[num_val:]\n",
        "\n",
        "# Show some data stats\n",
        "print(\"Number of training (src, trg) sentence pairs: %d\" %\n",
        "      len(train_src_sentences_list))\n",
        "print(\"Number of validation (src, trg) sentence pairs: %d\" %\n",
        "      len(val_src_sentences_list))\n",
        "print(\"Number of testing (src, trg) sentence pairs: %d\" %\n",
        "      len(test_src_sentences_list))\n",
        "src_vocab_set = ['<pad>'] + src_vocab_set\n",
        "trg_vocab_set = ['<pad>'] + trg_vocab_set\n",
        "print(\"Size of en vocab set (including '<pad>', '<unk>', '<s>', '</s>'): %d\" %\n",
        "      len(src_vocab_set))\n",
        "print(\"Size of vi vocab set (including '<pad>', '<unk>', '<s>', '</s>'): %d\" %\n",
        "      len(trg_vocab_set))\n",
        "\n",
        "length = [len(sent) for sent in train_src_sentences_list]\n",
        "print('Training sentence avg. length: %d ' % np.mean(length))\n",
        "print('Training sentence length at 95-percentile: %d' %\n",
        "      np.percentile(length, 95))\n",
        "print('Training sentence length distribution '\n",
        "      '(x-axis is length range and y-axis is count):\\n')\n",
        "plt.hist(length, bins=5)\n",
        "plt.show()\n",
        "\n",
        "print('Example Vietnamese input: ' + str(train_src_sentences_list[0]))\n",
        "print('Its target English output: ' + str(train_trg_sentences_list[0]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of training (src, trg) sentence pairs: 108748\n",
            "Number of validation (src, trg) sentence pairs: 12083\n",
            "Number of testing (src, trg) sentence pairs: 1139\n",
            "Size of en vocab set (including '<pad>', '<unk>', '<s>', '</s>'): 7710\n",
            "Size of vi vocab set (including '<pad>', '<unk>', '<s>', '</s>'): 17192\n",
            "Training sentence avg. length: 20 \n",
            "Training sentence length at 95-percentile: 42\n",
            "Training sentence length distribution (x-axis is length range and y-axis is count):\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYcAAAD4CAYAAAAHHSreAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Example Vietnamese input: ['Adam', 'Sadowsky', 'dàn', 'dựng', '1', 'video', 'âm', 'nhạc', 'hiện', 'tượng', '.']\n",
            "Its target English output: ['Adam', 'Sadowsky', ':', 'How', 'to', 'engineer', 'a', 'viral', 'music', 'video']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J2x0lhVm_Yxx",
        "colab_type": "text"
      },
      "source": [
        "Here we define a class called `MTDataset`. It is built on top of the efficient data loader API provided in PyTorch. See Section 5 for explanation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "muwDBzXM5ijT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from torch.utils import data\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "assert device == \"cuda\"   # use gpu whenever you can!\n",
        "\n",
        "seed = 42\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed(seed)\n",
        "\n",
        "\n",
        "# These IDs are reserved.\n",
        "PAD_INDEX = 0\n",
        "UNK_INDEX = 1\n",
        "SOS_INDEX = 2\n",
        "EOS_INDEX = 3\n",
        "\n",
        "\n",
        "class MTDataset(data.Dataset):\n",
        "  def __init__(self, src_sentences, src_vocabs, trg_sentences, trg_vocabs,\n",
        "               sampling=1.):\n",
        "    self.src_sentences = src_sentences[:int(len(src_sentences) * sampling)]\n",
        "    self.trg_sentences = trg_sentences[:int(len(src_sentences) * sampling)]\n",
        "\n",
        "    self.max_src_seq_length = MAX_SENT_LENGTH_PLUS_SOS_EOS\n",
        "    self.max_trg_seq_length = MAX_SENT_LENGTH_PLUS_SOS_EOS\n",
        "\n",
        "    self.src_vocabs = src_vocabs\n",
        "    self.trg_vocabs = trg_vocabs\n",
        "\n",
        "    self.src_v2id = {v : i for i, v in enumerate(src_vocabs)}\n",
        "    self.src_id2v = {val : key for key, val in self.src_v2id.items()}\n",
        "    self.trg_v2id = {v : i for i, v in enumerate(trg_vocabs)}\n",
        "    self.trg_id2v = {val : key for key, val in self.trg_v2id.items()}\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.src_sentences)\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    src_sent = self.src_sentences[index]\n",
        "    src_len = len(src_sent) + 2   # add <s> and </s> to each sentence\n",
        "    src_id = []\n",
        "    for w in src_sent:\n",
        "      if w not in self.src_vocabs:\n",
        "        w = '<unk>'\n",
        "      src_id.append(self.src_v2id[w])\n",
        "    src_id = ([SOS_INDEX] + src_id + [EOS_INDEX] + [PAD_INDEX] *\n",
        "              (self.max_src_seq_length - src_len))\n",
        "\n",
        "    trg_sent = self.trg_sentences[index]\n",
        "    trg_len = len(trg_sent) + 2\n",
        "    trg_id = []\n",
        "    for w in trg_sent:\n",
        "      if w not in self.trg_vocabs:\n",
        "        w = '<unk>'\n",
        "      trg_id.append(self.trg_v2id[w])\n",
        "    trg_id = ([SOS_INDEX] + trg_id + [EOS_INDEX] + [PAD_INDEX] *\n",
        "              (self.max_trg_seq_length - trg_len))\n",
        "\n",
        "    return torch.tensor(src_id), src_len, torch.tensor(trg_id), trg_len"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kb5gQEp7oVdi",
        "colab_type": "text"
      },
      "source": [
        "## **Section 2: Encoder**\n",
        "\n",
        "Seq2seq consists of an Encoder RNN and a decoder RNN. In a vanilla seq2seq model where there is no attention mechanism between encoder and decoder, the encoder aims to compress the information contained in the entire input sequence into a single vector and pass it to decoder.\n",
        "\n",
        "We start with implementing the encoder, which is just a simple RNN. We use a GRU here, but feel free to try other cell types."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dnwVGDVkoPt0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
        "\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "  def __init__(self, input_size, hidden_size, dropout=0.0, num_layers=1, bidirectional=False):\n",
        "    \"\"\"\n",
        "    Inputs: \n",
        "      - `input_size`: an int representing the RNN input size.\n",
        "      - `hidden_size`: an int representing the RNN hidden size.\n",
        "      - `dropout`: a float representing the dropout rate during training. Note\n",
        "          that for 1-layer RNN this has no effect since dropout only applies to\n",
        "          outputs of intermediate layers.\n",
        "    \"\"\"\n",
        "    super(Encoder, self).__init__()\n",
        "    self.input_size = input_size\n",
        "    self.hidden_size = hidden_size\n",
        "    self.dropout = dropout\n",
        "    self.num_layers = num_layers\n",
        "    self.num_directions = 2 if bidirectional else 1\n",
        "    # Note: for lab writeup question #4, you can directly change `num_layers`\n",
        "    # and `bidirectional` here to enable deep/bidirectional RNNs. However, you\n",
        "    # will also need to modify some parts in the rest of the code accordingly.\n",
        "    self.rnn = nn.GRU(input_size, hidden_size, num_layers=num_layers, batch_first=True,\n",
        "                      dropout=dropout, bidirectional=bidirectional)\n",
        "\n",
        "  def forward(self, inputs, lengths):\n",
        "    \"\"\"\n",
        "    Inputs:\n",
        "      - `inputs`: a 3d-tensor of shape (batch_size, max_seq_length, embed_size)\n",
        "          representing a batch of padded embedded word vectors of source\n",
        "          sentences.\n",
        "      - `lengths`: a 1d-tensor of shape (batch_size,) representing the sequence\n",
        "          lengths of `inputs`.\n",
        "\n",
        "    Returns:\n",
        "      - `outputs`: The outputs of the encoder RNN. A 3d-tensor of shape\n",
        "        (batch_size, max_seq_length, hidden_size).\n",
        "      - `finals`: The final hidden states of the encoder RNN. A 3d-tensor of \n",
        "          shape (num_layers * num_directions, batch_size, hidden_size).\n",
        "      Hint: `outputs` and `finals` are both standard GRU outputs. Check:\n",
        "      https://pytorch.org/docs/stable/nn.html#gru\n",
        "    \"\"\"\n",
        "    ### Your code here!\n",
        "    max_len = inputs.size(1)\n",
        "    packed_seq = pack_padded_sequence(inputs, lengths, batch_first=True, enforce_sorted=False)\n",
        "    outputs, finals = self.rnn(packed_seq)\n",
        "    outputs, lengths = pad_packed_sequence(outputs, batch_first=True, padding_value=PAD_INDEX, total_length=max_len)\n",
        "\n",
        "    return outputs, finals"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Oz3Kc4QKyEP",
        "colab_type": "text"
      },
      "source": [
        "## **Section 3: Decoder**\n",
        "\n",
        "Here you will implement a decoder RNN that uses encoder's last hidden state to initialize its initial hidden state."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JYT0BlfYUJXj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Decoder(nn.Module):\n",
        "  \"\"\"An RNN decoder without attention.\"\"\"\n",
        "\n",
        "  def __init__(self, input_size, hidden_size, dropout=0.0, num_layers=1, num_enc_layers=None):\n",
        "    \"\"\"\n",
        "      Inputs:\n",
        "        - `input_size`, `hidden_size`, and `dropout` the same as in Encoder.\n",
        "        - `num_enc_layers`: number of layers in the corresponding encoder.\n",
        "            When defined, creates a linear layer to pass the concatenation of\n",
        "            the encoder final hidden states `encoder_finals` to dim `hidden_size`\n",
        "            Equal to num_layers * num_directions in the encoder.\n",
        "            Default: `None`, in which case no linear layer is initialized and\n",
        "            number of encoder layers is assumed to be the same.\n",
        "    \"\"\"\n",
        "    super(Decoder, self).__init__()\n",
        "\n",
        "    ### Your code here!\n",
        "    self.input_size = input_size\n",
        "    self.hidden_size = hidden_size\n",
        "    self.dropout = dropout\n",
        "    self.num_layers = num_layers\n",
        "    self.num_enc_layers = num_enc_layers\n",
        "    if num_enc_layers is not None:\n",
        "      self.hidden_initializer = nn.Linear(num_enc_layers * hidden_size, num_layers * hidden_size)\n",
        "    else:\n",
        "      self.hidden_initializer = None\n",
        "\n",
        "    self.rnn = nn.GRU(input_size, hidden_size, num_layers=num_layers, batch_first=True,\n",
        "                      dropout=dropout, bidirectional=False)\n",
        "\n",
        "    \n",
        "  def forward(self, inputs, encoder_finals, hidden=None, max_len=None):\n",
        "    \"\"\"Unroll the decoder one step at a time.\n",
        "\n",
        "    Inputs:\n",
        "      - `inputs`: a 3d-tensor of shape (batch_size, max_seq_length-1, embed_size)\n",
        "          representing a batch of padded embedded word vectors of target\n",
        "          sentences (for teacher-forcing during training).\n",
        "      - `encoder_finals`: a 3d-tensor of shape\n",
        "          (num_enc_layers, batch_size, hidden_size) representing the final\n",
        "          encoder hidden states used to initialize the initial decoder hidden\n",
        "          states.\n",
        "      - `hidden`: a 3d-tensor of shape (1, batch_size, hidden_size) representing\n",
        "          the value to be used to initialize the initial decoder hidden states.\n",
        "          If None, then use `encoder_finals`.\n",
        "      - `max_len`: an int representing the maximum decoding length.\n",
        "\n",
        "    Returns:\n",
        "      - `outputs`: a 3d-tensor of shape\n",
        "          (batch_size, max_seq_length, hidden_size) representing the raw\n",
        "          decoder outputs (before converting to a `trg_vocab_size`-dim vector).\n",
        "          We will convert it later in a `Generator` below.\n",
        "      - `hidden`: a 3d-tensor of shape (1, batch_size, hidden_size)\n",
        "          representing the last decoder hidden state.\n",
        "    \"\"\"\n",
        "    if self.num_enc_layers is not None:\n",
        "      assert encoder_finals.size(0) == self.num_enc_layers, \\\n",
        "        'Decoder expected {} layer, but encoder_finals has {} layers.'.format(self.num_enc_layers, encoder_finals.size(0))\n",
        "    else:\n",
        "      assert encoder_finals.size(0) == self.num_layers, \\\n",
        "        'Decoder expected same number of layers by default, but encoder_finals has {} layers.'.format(self.num_layers, encoder_finals.size(0))\n",
        "\n",
        "    # The maximum number of steps to unroll the RNN.\n",
        "    if max_len is None:\n",
        "      max_len = inputs.size(1)\n",
        "\n",
        "    # Initialize decoder hidden state.\n",
        "    if hidden is None:\n",
        "      hidden = self.init_hidden(encoder_finals)\n",
        "\n",
        "    ### Your code here!\n",
        "    # outs = []\n",
        "    # for t in range(max_len):\n",
        "    #   prev = inputs[:, t:t+1, :]  # shape (batch_size, hidden_size)\n",
        "    #   out, hidden = self.rnn(prev, hidden)\n",
        "    #   outs.append(out)\n",
        "    # outputs = torch.cat(outs, dim=1)\n",
        "    outputs, hidden = self.rnn(inputs, hidden)\n",
        "\n",
        "    return hidden, outputs\n",
        "\n",
        "  def init_hidden(self, encoder_finals):\n",
        "    \"\"\"Use encoder final hidden state to initialize decoder's first hidden\n",
        "    state.\"\"\"\n",
        "    ### Your code here!\n",
        "    if self.hidden_initializer is None:\n",
        "      decoder_init_hiddens = encoder_finals\n",
        "    else:\n",
        "      finals_by_layer = tuple(encoder_finals)\n",
        "      finals_batch = torch.cat(finals_by_layer, dim=-1)  # shape: (batch_size, num_enc_layers * hidden_size)\n",
        "      out = self.hidden_initializer(finals_batch)  # shape: (batch_size, num_layers * hidden_size)\n",
        "      decoder_init_hiddens = torch.stack(torch.chunk(out, self.num_layers, dim=-1))\n",
        "\n",
        "    return decoder_init_hiddens"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AH0VdHE2_x1k",
        "colab_type": "text"
      },
      "source": [
        "Define the high level encoder-decoder class to wrap up sub-models, including encoder, decoder, generator, and src/trg embeddings."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nNBaAYB_oHxG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class EncoderDecoder(nn.Module):\n",
        "  \"\"\"A standard Encoder-Decoder architecture without attention.\n",
        "  \"\"\"\n",
        "  def __init__(self, encoder, decoder, src_embed, trg_embed, generator):\n",
        "    \"\"\"\n",
        "    Inputs:\n",
        "      - `encoder`: an `Encoder` object.\n",
        "      - `decoder`: an `Decoder` object.\n",
        "      - `src_embed`: an nn.Embedding object representing the lookup table for\n",
        "          input (source) sentences.\n",
        "      - `trg_embed`: an nn.Embedding object representing the lookup table for\n",
        "          output (target) sentences.\n",
        "      - `generator`: a `Generator` object. Essentially a linear mapping. See\n",
        "          the next code cell.\n",
        "    \"\"\"\n",
        "    super(EncoderDecoder, self).__init__()\n",
        "\n",
        "    self.encoder = encoder\n",
        "    self.decoder = decoder\n",
        "    self.src_embed = src_embed\n",
        "    self.trg_embed = trg_embed\n",
        "    self.generator = generator\n",
        "\n",
        "  def forward(self, src_ids, trg_ids, src_lengths):\n",
        "    \"\"\"Take in and process masked source and target sequences.\n",
        "\n",
        "    Inputs:\n",
        "      `src_ids`: a 2d-tensor of shape (batch_size, max_seq_length) representing\n",
        "        a batch of source sentences of word ids.\n",
        "      `trg_ids`: a 2d-tensor of shape (batch_size, max_seq_length) representing\n",
        "        a batch of target sentences of word ids.\n",
        "      `src_lengths`: a 1d-tensor of shape (batch_size,) representing the\n",
        "        sequence length of `src_ids`.\n",
        "\n",
        "    Returns the decoder outputs, see the above cell.\n",
        "    \"\"\"\n",
        "    encoder_hiddens, encoder_finals = self.encode(src_ids, src_lengths)\n",
        "    del encoder_hiddens   # unused\n",
        "    return self.decode(trg_ids[:, :-1], encoder_finals)\n",
        "\n",
        "  def encode(self, src_ids, src_lengths):\n",
        "    return self.encoder(self.src_embed(src_ids), src_lengths)\n",
        "    \n",
        "  def decode(self, trg_id_inputs, encoder_finals, encoder_hiddens=None, src_mask=None, trg_mask=None, decoder_hidden=None):\n",
        "    return self.decoder(self.trg_embed(trg_id_inputs), encoder_finals, decoder_hidden)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M06QOTbCALGy",
        "colab_type": "text"
      },
      "source": [
        "It simply projects the pre-output layer (x in the forward function below) to obtain the output layer, so that the final dimension is the target vocabulary size."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LaHdVcF1KPmd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Generator(nn.Module):\n",
        "  \"\"\"Define standard linear + softmax generation step.\"\"\"\n",
        "  def __init__(self, hidden_size, vocab_size):\n",
        "    super(Generator, self).__init__()\n",
        "    self.proj = nn.Linear(hidden_size, vocab_size, bias=False)\n",
        "\n",
        "  def forward(self, x):\n",
        "    return F.log_softmax(self.proj(x), dim=-1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qf8Oc9a_ocC_",
        "colab_type": "text"
      },
      "source": [
        "## **Section 4: Attention-Based Decoder**\n",
        "\n",
        "Now it's time to add some attention to the decoder. You can implement any attention mechanism you want.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X9reZZD_-xlK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class GlobalAttention(nn.Module):\n",
        "  \"\"\"A global attention layer, as introduced by Luong et al. (2015)\"\"\"\n",
        "\n",
        "  def __init__(self, hidden_size):\n",
        "    \"\"\"\n",
        "    \"\"\"\n",
        "    super(GlobalAttention, self).__init__()\n",
        "\n",
        "    self.score_weights = nn.Parameter(torch.Tensor(hidden_size, hidden_size))\n",
        "    self.attn_softmax = nn.Softmax(dim=-1)\n",
        "    self.concat = nn.Linear(hidden_size * 2, hidden_size)\n",
        "    self.out = nn.Tanh()\n",
        "    self.reset_parameters()\n",
        "  \n",
        "  def reset_parameters(self):\n",
        "    \"\"\"Reset the score matrix weights\"\"\"\n",
        "    nn.init.kaiming_uniform_(self.score_weights, a=math.sqrt(5))  # same method for nn.Linear.weight init\n",
        "\n",
        "  def forward(self, encoder_outputs, decoder_outputs, src_mask=None, trg_mask=None):\n",
        "    \"\"\"Calculate attention weights and resulting attention hidden states\n",
        "    using the attention-weighted sum of encoder outputs and the decoder_output.\n",
        "    \n",
        "    Inputs:\n",
        "      - `encoder_outputs`: a 3d-tensor of shape\n",
        "          (batch_size, max_seq_length, hidden_size) representing the encoder\n",
        "          outputs for each encoding step.\n",
        "      - `decoder_outputs`: a 3d-tensor of shape\n",
        "          (batch_size, max_seq_length-1, hidden_size) representing the decoder\n",
        "          outputs for each decoding step. \n",
        "      - `src_mask`: a 3d-tensor of shape (batch_size, 1, max_seq_length)\n",
        "          representing the mask for source sentences. If None, assume that\n",
        "          all words in the source sentences are relevant (e.g. not padding).\n",
        "          Default: `None`\n",
        "      - `trg_mask`: a 3d-tensor of shape (batch_size, 1, max_seq_length-1)\n",
        "          representing the mask for target sentences. Unused. Default: `None`\n",
        "\n",
        "    Returns:\n",
        "      - `outputs`: a 3d-tensor of shape\n",
        "          (batch_size, max_seq_length, hidden_size) representing the raw outputs\n",
        "          of the attention layer (before converting to a `trg_vocab_size`-dim vector).\n",
        "    \"\"\"\n",
        "    max_len = decoder_outputs.size(1)\n",
        "    # shape (batch_size, max_seq_length, max_seq_length).\n",
        "    # dim=1 for each decoder output, dim=2 for each encoder output\n",
        "    scores = torch.bmm(decoder_outputs@self.score_weights, torch.transpose(encoder_outputs, 1, 2))\n",
        "    if src_mask is not None:\n",
        "      attn_mask = src_mask.expand(-1, max_len, -1)\n",
        "      scores = scores + torch.log(attn_mask)\n",
        "    attn_weights = self.attn_softmax(scores)\n",
        "    context = torch.bmm(attn_weights, encoder_outputs)  # shape (batch_size, max_seq_length, hidden_size)\n",
        "    attn_hidden = self.out(self.concat(torch.cat((decoder_outputs, context), dim=-1)))\n",
        "    return attn_hidden"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iZq2NImAoY1C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class AttentionDecoder(nn.Module):\n",
        "  \"\"\"An attention-based RNN decoder.\"\"\"\n",
        "\n",
        "  def __init__(self, input_size, hidden_size, attention=None, dropout=0.0, num_layers=1, num_enc_layers=None):\n",
        "    \"\"\"\n",
        "      Inputs:\n",
        "        - `input_size`, `hidden_size`, and `dropout` the same as in Encoder.\n",
        "        - `attention`: this is your self-defined Attention object. You can\n",
        "            either define an individual class for your Attention and pass it\n",
        "            here or leave `attention` as None and just implement everything\n",
        "            here.\n",
        "        - `num_enc_layers`: number of layers in the corresponding encoder.\n",
        "            When defined, creates a linear layer to pass the concatenation of\n",
        "            the encoder final hidden states `encoder_finals` to dim `hidden_size`\n",
        "            Equal to num_layers * num_directions in the encoder.\n",
        "            Default: `None`, in which case no linear layer is initialized and\n",
        "            number of encoder layers is assumed to be the same.\n",
        "    \"\"\"\n",
        "    super(AttentionDecoder, self).__init__()\n",
        "\n",
        "    ### Your code here!\n",
        "    self.input_size = input_size\n",
        "    self.hidden_size = hidden_size\n",
        "    self.attention = attention\n",
        "    self.dropout = dropout\n",
        "    self.num_layers = num_layers\n",
        "    self.num_enc_layers = num_enc_layers\n",
        "    if num_enc_layers is not None:\n",
        "      self.hidden_initializer = nn.Linear(num_enc_layers * hidden_size, num_layers * hidden_size)\n",
        "    else:\n",
        "      self.hidden_initializer = None\n",
        "\n",
        "    self.rnn = nn.GRU(input_size, hidden_size, num_layers=num_layers, batch_first=True,\n",
        "                      dropout=dropout, bidirectional=False)\n",
        "\n",
        "    \n",
        "  def forward(self, inputs, encoder_hiddens, encoder_finals, src_mask,\n",
        "              trg_mask, hidden=None, max_len=None):\n",
        "    \"\"\"Unroll the decoder one step at a time.\n",
        "    \n",
        "    Inputs:\n",
        "      - `inputs`: a 3d-tensor of shape (batch_size, max_seq_length-1, embed_size)\n",
        "          representing a batch of padded embedded word vectors of target\n",
        "          sentences (for teacher-forcing during training).\n",
        "      - `encoder_hiddens`: a 3d-tensor of shape\n",
        "          (batch_size, max_seq_length, hidden_size) representing the encoder\n",
        "          outputs for each decoding step to attend to. \n",
        "      - `encoder_finals`: a 3d-tensor of shape\n",
        "          (num_enc_layers, batch_size, hidden_size) representing the final\n",
        "          encoder hidden states used to initialize the initial decoder hidden\n",
        "          states.\n",
        "      - `src_mask`: a 3d-tensor of shape (batch_size, 1, max_seq_length)\n",
        "          representing the mask for source sentences.\n",
        "      - `trg_mask`: a 3d-tensor of shape (batch_size, 1, max_seq_length-1)\n",
        "          representing the mask for target sentences.\n",
        "      - `hidden`: a 3d-tensor of shape (1, batch_size, hidden_size) representing\n",
        "          the value to be used to initialize the initial decoder hidden states.\n",
        "          If None, then use `encoder_finals`.\n",
        "      - `max_len`: an int representing the maximum decoding length.\n",
        "\n",
        "    Returns:\n",
        "      - `outputs`: (same as in Decoder) a 3d-tensor of shape\n",
        "          (batch_size, max_seq_length, hidden_size) representing the raw\n",
        "          decoder outputs (before converting to a `trg_vocab_size`-dim vector).\n",
        "      - `hidden`: a 3d-tensor of shape (1, batch_size, hidden_size)\n",
        "          representing the last decoder hidden state.\n",
        "    \"\"\"\n",
        "    if self.num_enc_layers is not None:\n",
        "      assert encoder_finals.size(0) == self.num_enc_layers, \\\n",
        "        'Decoder expected {} layer, but encoder_finals has {} layers.'.format(self.num_enc_layers, encoder_finals.size(0))\n",
        "    else:\n",
        "      assert encoder_finals.size(0) == self.num_layers, \\\n",
        "        'Decoder expected same number of layers by default, but encoder_finals has {} layers.'.format(self.num_layers, encoder_finals.size(0))\n",
        "\n",
        "    # The maximum number of steps to unroll the RNN.\n",
        "    if max_len is None:\n",
        "      max_len = inputs.size(1)\n",
        "\n",
        "    if hidden is None:\n",
        "      hidden = self.init_hidden(encoder_finals)\n",
        "\n",
        "    outputs = None \n",
        "    ### Your code here!\n",
        "    decoder_hiddens, hidden = self.rnn(inputs, hidden)\n",
        "    if self.attention is not None:\n",
        "      outputs = self.attention(encoder_hiddens, decoder_hiddens, src_mask, trg_mask)\n",
        "    else:\n",
        "      outputs = decoder_hiddens\n",
        "\n",
        "    return hidden, outputs\n",
        "\n",
        "  def init_hidden(self, encoder_finals):\n",
        "    \"\"\"Use encoder final hidden state to initialize decoder's first hidden\n",
        "    state.\"\"\"\n",
        "    ### Your code here!\n",
        "    if self.hidden_initializer is None:\n",
        "      decoder_init_hiddens = encoder_finals\n",
        "    else:\n",
        "      finals_by_layer = tuple(encoder_finals)\n",
        "      finals_batch = torch.cat(finals_by_layer, dim=-1)  # shape: (batch_size, num_enc_layers * hidden_size)\n",
        "      out = self.hidden_initializer(finals_batch)  # shape: (batch_size, num_layers * hidden_size)\n",
        "      decoder_init_hiddens = torch.stack(torch.chunk(out, self.num_layers, dim=-1))\n",
        "\n",
        "    return decoder_init_hiddens"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DTUXxJWPPQ9W",
        "colab_type": "text"
      },
      "source": [
        "Similarly, we use a `EncoderAttentionDecoder` class to wrap up all encoder, decoder, src/trg embeddings, and generator. You can take the `EncoderDecoder` class as a reference."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mghIa6XzubZL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class EncoderAttentionDecoder(nn.Module):\n",
        "  \"\"\"A Encoder-Decoder architecture with attention.\n",
        "  \"\"\"\n",
        "  def __init__(self, encoder, decoder, src_embed , trg_embed, generator):\n",
        "    \"\"\"\n",
        "    Inputs:\n",
        "      - `encoder`: an `Encoder` object.\n",
        "      - `decoder`: an `AttentionDecoder` object.\n",
        "      - `src_embed`: an nn.Embedding object representing the lookup table for\n",
        "          input (source) sentences.\n",
        "      - `trg_embed`: an nn.Embedding object representing the lookup table for\n",
        "          output (target) sentences.\n",
        "      - `generator`: a `Generator` object. Essentially a linear mapping. See\n",
        "          the next code cell.\n",
        "    \"\"\"\n",
        "    super(EncoderAttentionDecoder, self).__init__()\n",
        "\n",
        "    self.encoder = encoder\n",
        "    self.decoder = decoder\n",
        "    self.src_embed = src_embed\n",
        "    self.trg_embed = trg_embed\n",
        "    self.generator = generator\n",
        "\n",
        "  def forward(self, src_ids, trg_ids, src_lengths):\n",
        "    \"\"\"Take in and process masked source and tar get sequences.\n",
        "\n",
        "    Inputs:\n",
        "      `src_ids`: a 2d-tensor of shape (batch_size, max_seq_length) representing\n",
        "        a batch of source sentences of word ids.\n",
        "      `trg_ids`: a 2d-tensor of shape (batch_size, max_seq_length) representing\n",
        "        a batch of target sentences of word ids.\n",
        "      `src_lengths`: a 1d-tensor of shape (batch_size,) representing the\n",
        "        sequence length of `src_ids`.\n",
        "\n",
        "    Returns the decoder outputs, see the above cell.\n",
        "    \"\"\"\n",
        "    ### Your code here!\n",
        "    # You can refer to `EncoderDecoder` and extend from it.\n",
        "    trg_id_inputs = trg_ids[:, :-1]  # skip last id b/c it won't be an input for a prediction\n",
        "    src_mask = torch.where(src_ids == PAD_INDEX, torch.zeros(src_ids.size()).to(device), torch.ones(src_ids.size()).to(device))\n",
        "    src_mask.unsqueeze_(1)\n",
        "    trg_mask = torch.where(trg_id_inputs == PAD_INDEX, torch.zeros(trg_id_inputs.size()).to(device), torch.ones(trg_id_inputs.size()).to(device))\n",
        "    trg_mask.unsqueeze_(1)\n",
        "    encoder_hiddens, encoder_finals = self.encode(src_ids, src_lengths)\n",
        "    return self.decode(trg_id_inputs, encoder_finals, encoder_hiddens, src_mask, trg_mask)\n",
        "\n",
        "  def encode(self, src_ids, src_lengths):\n",
        "    return self.encoder(self.src_embed(src_ids), src_lengths)\n",
        "    \n",
        "  def decode(self, trg_id_inputs, encoder_finals, encoder_hiddens, src_mask=None, trg_mask=None, decoder_hidden=None):\n",
        "    return self.decoder(self.trg_embed(trg_id_inputs), encoder_hiddens, encoder_finals, src_mask, trg_mask, hidden=decoder_hidden)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9VIpNlKtK8l_",
        "colab_type": "text"
      },
      "source": [
        "## **Section 5: Training and Testing**\n",
        "\n",
        "We provide training and testing scripts here. You might need to adapt them to fit your model implementation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I38IFq48BKa5",
        "colab_type": "text"
      },
      "source": [
        "Apply the dataloader to the MT dataset. Dataloader provides a convenient way to iterate through the whole dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cJrXO7nCjzBP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_size = 128\n",
        "\n",
        "# You can try on a smaller training set by setting a smaller `sampling`.\n",
        "train_set = MTDataset(train_src_sentences_list, src_vocab_set,\n",
        "                      train_trg_sentences_list, trg_vocab_set, sampling=1.)\n",
        "train_data_loader = data.DataLoader(train_set, batch_size=batch_size,\n",
        "                                    num_workers=8, shuffle=True)\n",
        "\n",
        "val_set = MTDataset(val_src_sentences_list, src_vocab_set,\n",
        "                    val_trg_sentences_list, trg_vocab_set, sampling=1.)\n",
        "val_data_loader = data.DataLoader(val_set, batch_size=batch_size, num_workers=8,\n",
        "                                  shuffle=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bWaiu7wNBX7x",
        "colab_type": "text"
      },
      "source": [
        "The main functions for training, here we use perplexity to evaluate the performance of the model. Although we provide the training scripts here, we strongly encourage you to go through and understand the procedure."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AXGa-L1qp13q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import math\n",
        "\n",
        "\n",
        "class SimpleLossCompute:\n",
        "  \"\"\"A simple loss compute and train function.\"\"\"\n",
        "\n",
        "  def __init__(self, generator, criterion, opt=None):\n",
        "    self.generator = generator\n",
        "    self.criterion = criterion\n",
        "    self.opt = opt\n",
        "\n",
        "  def __call__(self, x, y, norm):\n",
        "    x = self.generator(x)\n",
        "    loss = self.criterion(x.contiguous().view(-1, x.size(-1)),\n",
        "                          y.contiguous().view(-1))\n",
        "    loss = loss / norm\n",
        "\n",
        "    if self.opt is not None:  # training mode\n",
        "      loss.backward()          \n",
        "      self.opt.step()\n",
        "      self.opt.zero_grad()\n",
        "\n",
        "    return loss.data.item() * norm\n",
        "\n",
        "\n",
        "def run_epoch(data_loader, model, loss_compute, print_every):\n",
        "  \"\"\"Standard Training and Logging Function\"\"\"\n",
        "\n",
        "  total_tokens = 0\n",
        "  total_loss = 0\n",
        "\n",
        "  for i, (src_ids_BxT, src_lengths_B, trg_ids_BxL, trg_lengths_B) in enumerate(data_loader):\n",
        "    # We define some notations here to help you understand the loaded tensor\n",
        "    # shapes:\n",
        "    #   `B`: batch size\n",
        "    #   `T`: max sequence length of source sentences\n",
        "    #   `L`: max sequence length of target sentences; due to our preprocessing\n",
        "    #        in the beginning, `L` == `T` == 50\n",
        "    # An example of `src_ids_BxT` (when B = 2):\n",
        "    #   [[2, 4, 6, 7, ..., 4, 3, 0, 0, 0],\n",
        "    #    [2, 8, 6, 5, ..., 9, 5, 4, 3, 0]]\n",
        "    # The corresponding `src_lengths_B` would be [47, 49].\n",
        "    # Note that SOS_INDEX == 2, EOS_INDEX == 3, and PAD_INDEX = 0.\n",
        "\n",
        "    src_ids_BxT = src_ids_BxT.to(device)\n",
        "    src_lengths_B = src_lengths_B.to(device)\n",
        "    trg_ids_BxL = trg_ids_BxL.to(device)\n",
        "    del trg_lengths_B   # unused\n",
        "\n",
        "    _, output = model(src_ids_BxT, trg_ids_BxL, src_lengths_B)\n",
        "\n",
        "    loss = loss_compute(x=output, y=trg_ids_BxL[:, 1:],\n",
        "                        norm=src_ids_BxT.size(0))\n",
        "    total_loss += loss\n",
        "    total_tokens += (trg_ids_BxL[:, 1:] != PAD_INDEX).data.sum().item()\n",
        "\n",
        "    if model.training and i % print_every == 0:\n",
        "      print(\"Epoch Step: %d Loss: %f\" % (i, loss / src_ids_BxT.size(0)))\n",
        "\n",
        "  return math.exp(total_loss / float(total_tokens))\n",
        "\n",
        "\n",
        "def train(model, num_epochs, learning_rate, print_every):\n",
        "  # Set `ignore_index` as PAD_INDEX so that pad tokens won't be included when\n",
        "  # computing the loss.\n",
        "  criterion = nn.NLLLoss(reduction=\"sum\", ignore_index=PAD_INDEX)\n",
        "  optim = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "  # Keep track of dev ppl for each epoch.\n",
        "  dev_ppls = []\n",
        "\n",
        "  for epoch in range(num_epochs):\n",
        "    print(\"Epoch\", epoch)\n",
        "\n",
        "    model.train()\n",
        "    train_ppl = run_epoch(data_loader=train_data_loader, model=model,\n",
        "                          loss_compute=SimpleLossCompute(model.generator,\n",
        "                                                         criterion, optim),\n",
        "                          print_every=print_every)\n",
        "        \n",
        "    model.eval()\n",
        "    with torch.no_grad():      \n",
        "      dev_ppl = run_epoch(data_loader=val_data_loader, model=model,\n",
        "                          loss_compute=SimpleLossCompute(model.generator,\n",
        "                                                         criterion, None),\n",
        "                          print_every=print_every)\n",
        "      print(\"Validation perplexity: %f\" % dev_ppl)\n",
        "      dev_ppls.append(dev_ppl)\n",
        "        \n",
        "  return dev_ppls"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1A8VvkcICT60",
        "colab_type": "text"
      },
      "source": [
        "The main function to perform training. First let's train the vanilla seq2seq model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "03_J2vRnCXE4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Hyperparameters for constructing the encoder-decoder model.\n",
        "\n",
        "embed_size = 256   # Each word will be represented as a `embed_size`-dim vector.\n",
        "hidden_size = 256  # RNN hidden size.\n",
        "dropout = 0.2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pZ0t1hXAIHtO",
        "colab_type": "code",
        "outputId": "5183d0a9-9723-4714-b32d-3c5372daf085",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "pure_seq2seq = EncoderDecoder(\n",
        "  encoder=Encoder(embed_size, hidden_size, dropout=dropout),\n",
        "  decoder=Decoder(embed_size, hidden_size, dropout=dropout),\n",
        "  src_embed=nn.Embedding(len(src_vocab_set), embed_size),\n",
        "  trg_embed=nn.Embedding(len(trg_vocab_set), embed_size),\n",
        "  generator=Generator(hidden_size, len(trg_vocab_set))).to(device)\n",
        "\n",
        "# Start training. The returned `dev_ppls` is a list of dev perplexity for each\n",
        "# epoch.\n",
        "pure_dev_ppls = train(pure_seq2seq, num_epochs=10, learning_rate=1e-3,\n",
        "                      print_every=100)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/rnn.py:50: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  \"num_layers={}\".format(dropout, num_layers))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 0\n",
            "Epoch Step: 0 Loss: 180.633118\n",
            "Epoch Step: 100 Loss: 101.318810\n",
            "Epoch Step: 200 Loss: 89.718628\n",
            "Epoch Step: 300 Loss: 89.796783\n",
            "Epoch Step: 400 Loss: 84.137215\n",
            "Epoch Step: 500 Loss: 82.346848\n",
            "Epoch Step: 600 Loss: 82.064011\n",
            "Epoch Step: 700 Loss: 78.606743\n",
            "Epoch Step: 800 Loss: 73.170242\n",
            "Validation perplexity: 71.565741\n",
            "Epoch 1\n",
            "Epoch Step: 0 Loss: 70.690102\n",
            "Epoch Step: 100 Loss: 71.882896\n",
            "Epoch Step: 200 Loss: 65.996017\n",
            "Epoch Step: 300 Loss: 74.677277\n",
            "Epoch Step: 400 Loss: 70.145180\n",
            "Epoch Step: 500 Loss: 70.731873\n",
            "Epoch Step: 600 Loss: 61.844410\n",
            "Epoch Step: 700 Loss: 73.761009\n",
            "Epoch Step: 800 Loss: 72.441727\n",
            "Validation perplexity: 52.571025\n",
            "Epoch 2\n",
            "Epoch Step: 0 Loss: 71.767685\n",
            "Epoch Step: 100 Loss: 61.507153\n",
            "Epoch Step: 200 Loss: 69.426308\n",
            "Epoch Step: 300 Loss: 63.468651\n",
            "Epoch Step: 400 Loss: 67.633392\n",
            "Epoch Step: 500 Loss: 68.163879\n",
            "Epoch Step: 600 Loss: 69.368004\n",
            "Epoch Step: 700 Loss: 72.594017\n",
            "Epoch Step: 800 Loss: 60.646427\n",
            "Validation perplexity: 45.712289\n",
            "Epoch 3\n",
            "Epoch Step: 0 Loss: 63.422913\n",
            "Epoch Step: 100 Loss: 61.546204\n",
            "Epoch Step: 200 Loss: 61.686073\n",
            "Epoch Step: 300 Loss: 60.562504\n",
            "Epoch Step: 400 Loss: 62.624077\n",
            "Epoch Step: 500 Loss: 65.982780\n",
            "Epoch Step: 600 Loss: 64.206978\n",
            "Epoch Step: 700 Loss: 63.498661\n",
            "Epoch Step: 800 Loss: 61.624420\n",
            "Validation perplexity: 42.563565\n",
            "Epoch 4\n",
            "Epoch Step: 0 Loss: 56.215462\n",
            "Epoch Step: 100 Loss: 57.366138\n",
            "Epoch Step: 200 Loss: 64.786446\n",
            "Epoch Step: 300 Loss: 57.875740\n",
            "Epoch Step: 400 Loss: 52.595833\n",
            "Epoch Step: 500 Loss: 62.421452\n",
            "Epoch Step: 600 Loss: 60.782154\n",
            "Epoch Step: 700 Loss: 54.303741\n",
            "Epoch Step: 800 Loss: 55.162956\n",
            "Validation perplexity: 40.898937\n",
            "Epoch 5\n",
            "Epoch Step: 0 Loss: 55.570705\n",
            "Epoch Step: 100 Loss: 54.430317\n",
            "Epoch Step: 200 Loss: 60.046745\n",
            "Epoch Step: 300 Loss: 56.597809\n",
            "Epoch Step: 400 Loss: 63.134739\n",
            "Epoch Step: 500 Loss: 53.366535\n",
            "Epoch Step: 600 Loss: 53.062328\n",
            "Epoch Step: 700 Loss: 64.596329\n",
            "Epoch Step: 800 Loss: 52.773830\n",
            "Validation perplexity: 40.180999\n",
            "Epoch 6\n",
            "Epoch Step: 0 Loss: 48.504921\n",
            "Epoch Step: 100 Loss: 51.596447\n",
            "Epoch Step: 200 Loss: 53.161747\n",
            "Epoch Step: 300 Loss: 55.513275\n",
            "Epoch Step: 400 Loss: 55.887405\n",
            "Epoch Step: 500 Loss: 54.406578\n",
            "Epoch Step: 600 Loss: 53.540848\n",
            "Epoch Step: 700 Loss: 50.793667\n",
            "Epoch Step: 800 Loss: 51.251999\n",
            "Validation perplexity: 40.093081\n",
            "Epoch 7\n",
            "Epoch Step: 0 Loss: 53.987984\n",
            "Epoch Step: 100 Loss: 49.185349\n",
            "Epoch Step: 200 Loss: 47.281414\n",
            "Epoch Step: 300 Loss: 55.687019\n",
            "Epoch Step: 400 Loss: 47.568256\n",
            "Epoch Step: 500 Loss: 57.264889\n",
            "Epoch Step: 600 Loss: 48.946930\n",
            "Epoch Step: 700 Loss: 51.297493\n",
            "Epoch Step: 800 Loss: 55.053127\n",
            "Validation perplexity: 40.345104\n",
            "Epoch 8\n",
            "Epoch Step: 0 Loss: 48.389885\n",
            "Epoch Step: 100 Loss: 47.893730\n",
            "Epoch Step: 200 Loss: 49.985355\n",
            "Epoch Step: 300 Loss: 47.184898\n",
            "Epoch Step: 400 Loss: 51.464222\n",
            "Epoch Step: 500 Loss: 52.233479\n",
            "Epoch Step: 600 Loss: 55.290817\n",
            "Epoch Step: 700 Loss: 50.430946\n",
            "Epoch Step: 800 Loss: 53.132641\n",
            "Validation perplexity: 41.121290\n",
            "Epoch 9\n",
            "Epoch Step: 0 Loss: 41.018993\n",
            "Epoch Step: 100 Loss: 46.950047\n",
            "Epoch Step: 200 Loss: 48.563366\n",
            "Epoch Step: 300 Loss: 54.644745\n",
            "Epoch Step: 400 Loss: 43.598812\n",
            "Epoch Step: 500 Loss: 44.373650\n",
            "Epoch Step: 600 Loss: 47.747013\n",
            "Epoch Step: 700 Loss: 44.892754\n",
            "Epoch Step: 800 Loss: 44.577843\n",
            "Validation perplexity: 41.971095\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fRsfDg0wCa7U",
        "colab_type": "text"
      },
      "source": [
        "Plot the perplexity graph."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CTApnlT53YvT",
        "colab_type": "code",
        "outputId": "d36a420a-7ad8-48f6-a939-dc8523e3eb25",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        }
      },
      "source": [
        "def plot_perplexity(perplexities):\n",
        "  \"\"\"plot perplexities\"\"\"\n",
        "  plt.title(\"Perplexity per Epoch\")\n",
        "  plt.xlabel(\"Epoch\")\n",
        "  plt.ylabel(\"Perplexity\")\n",
        "  plt.plot(perplexities)\n",
        "\n",
        "plot_perplexity(pure_dev_ppls)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAEWCAYAAABhffzLAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nf2+y5Ngu2rAgAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PjhKXDyvIk4l",
        "colab_type": "text"
      },
      "source": [
        "Now, let's train the seq2seq model with attention."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "onQxDU-aGa0t",
        "colab_type": "code",
        "outputId": "f66f9c6a-8283-4c6f-de8b-d7aa0506c92a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "attn_seq2seq = EncoderAttentionDecoder(\n",
        "  encoder=Encoder(embed_size, hidden_size, dropout=dropout),\n",
        "  decoder=AttentionDecoder(embed_size, hidden_size,\n",
        "                  attention=GlobalAttention(hidden_size), dropout=dropout),\n",
        "  src_embed=nn.Embedding(len(src_vocab_set), embed_size),\n",
        "  trg_embed=nn.Embedding(len(trg_vocab_set), embed_size),\n",
        "  generator=Generator(hidden_size, len(trg_vocab_set))).to(device)\n",
        "\n",
        "attn_dev_ppls = train(attn_seq2seq, num_epochs=10, learning_rate=1e-3,\n",
        "                      print_every=100)\n",
        "\n",
        "plot_perplexity(attn_dev_ppls)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/rnn.py:50: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  \"num_layers={}\".format(dropout, num_layers))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch Step: 0 Loss: 189.785309\n",
            "Epoch Step: 100 Loss: 98.854095\n",
            "Epoch Step: 200 Loss: 93.748451\n",
            "Epoch Step: 300 Loss: 101.221603\n",
            "Epoch Step: 400 Loss: 82.597069\n",
            "Epoch Step: 500 Loss: 81.971451\n",
            "Epoch Step: 600 Loss: 78.224091\n",
            "Epoch Step: 700 Loss: 73.723450\n",
            "Epoch Step: 800 Loss: 66.977295\n",
            "Validation perplexity: 54.953826\n",
            "Epoch 1\n",
            "Epoch Step: 0 Loss: 68.943909\n",
            "Epoch Step: 100 Loss: 76.346100\n",
            "Epoch Step: 200 Loss: 66.115692\n",
            "Epoch Step: 300 Loss: 69.452316\n",
            "Epoch Step: 400 Loss: 68.290710\n",
            "Epoch Step: 500 Loss: 53.402336\n",
            "Epoch Step: 600 Loss: 66.281624\n",
            "Epoch Step: 700 Loss: 65.393776\n",
            "Epoch Step: 800 Loss: 61.161064\n",
            "Validation perplexity: 32.057452\n",
            "Epoch 2\n",
            "Epoch Step: 0 Loss: 60.615322\n",
            "Epoch Step: 100 Loss: 60.718708\n",
            "Epoch Step: 200 Loss: 58.456387\n",
            "Epoch Step: 300 Loss: 56.053986\n",
            "Epoch Step: 400 Loss: 56.529404\n",
            "Epoch Step: 500 Loss: 57.057533\n",
            "Epoch Step: 600 Loss: 58.358150\n",
            "Epoch Step: 700 Loss: 53.236725\n",
            "Epoch Step: 800 Loss: 58.846054\n",
            "Validation perplexity: 24.499265\n",
            "Epoch 3\n",
            "Epoch Step: 0 Loss: 54.113281\n",
            "Epoch Step: 100 Loss: 48.867916\n",
            "Epoch Step: 200 Loss: 52.079895\n",
            "Epoch Step: 300 Loss: 53.885483\n",
            "Epoch Step: 400 Loss: 48.109226\n",
            "Epoch Step: 500 Loss: 53.201221\n",
            "Epoch Step: 600 Loss: 50.264198\n",
            "Epoch Step: 700 Loss: 53.768120\n",
            "Epoch Step: 800 Loss: 48.520626\n",
            "Validation perplexity: 21.180149\n",
            "Epoch 4\n",
            "Epoch Step: 0 Loss: 44.991894\n",
            "Epoch Step: 100 Loss: 44.620190\n",
            "Epoch Step: 200 Loss: 46.411469\n",
            "Epoch Step: 300 Loss: 48.106785\n",
            "Epoch Step: 400 Loss: 51.304707\n",
            "Epoch Step: 500 Loss: 47.382576\n",
            "Epoch Step: 600 Loss: 50.152821\n",
            "Epoch Step: 700 Loss: 47.297447\n",
            "Epoch Step: 800 Loss: 48.895927\n",
            "Validation perplexity: 19.386287\n",
            "Epoch 5\n",
            "Epoch Step: 0 Loss: 44.388580\n",
            "Epoch Step: 100 Loss: 43.173389\n",
            "Epoch Step: 200 Loss: 38.921841\n",
            "Epoch Step: 300 Loss: 46.252361\n",
            "Epoch Step: 400 Loss: 41.548759\n",
            "Epoch Step: 500 Loss: 45.348408\n",
            "Epoch Step: 600 Loss: 51.669090\n",
            "Epoch Step: 700 Loss: 49.087471\n",
            "Epoch Step: 800 Loss: 38.243607\n",
            "Validation perplexity: 18.638537\n",
            "Epoch 6\n",
            "Epoch Step: 0 Loss: 42.013371\n",
            "Epoch Step: 100 Loss: 42.229362\n",
            "Epoch Step: 200 Loss: 39.826702\n",
            "Epoch Step: 300 Loss: 40.224667\n",
            "Epoch Step: 400 Loss: 37.466419\n",
            "Epoch Step: 500 Loss: 43.595734\n",
            "Epoch Step: 600 Loss: 40.155891\n",
            "Epoch Step: 700 Loss: 37.043289\n",
            "Epoch Step: 800 Loss: 40.752186\n",
            "Validation perplexity: 18.003459\n",
            "Epoch 7\n",
            "Epoch Step: 0 Loss: 39.375031\n",
            "Epoch Step: 100 Loss: 38.774548\n",
            "Epoch Step: 200 Loss: 33.038891\n",
            "Epoch Step: 300 Loss: 39.094501\n",
            "Epoch Step: 400 Loss: 45.765572\n",
            "Epoch Step: 500 Loss: 38.387550\n",
            "Epoch Step: 600 Loss: 37.737862\n",
            "Epoch Step: 700 Loss: 39.508648\n",
            "Epoch Step: 800 Loss: 42.848141\n",
            "Validation perplexity: 17.887577\n",
            "Epoch 8\n",
            "Epoch Step: 0 Loss: 40.197399\n",
            "Epoch Step: 100 Loss: 35.573639\n",
            "Epoch Step: 200 Loss: 38.930840\n",
            "Epoch Step: 300 Loss: 40.791447\n",
            "Epoch Step: 400 Loss: 38.996529\n",
            "Epoch Step: 500 Loss: 39.604721\n",
            "Epoch Step: 600 Loss: 34.726109\n",
            "Epoch Step: 700 Loss: 39.164337\n",
            "Epoch Step: 800 Loss: 38.615520\n",
            "Validation perplexity: 17.865742\n",
            "Epoch 9\n",
            "Epoch Step: 0 Loss: 32.348057\n",
            "Epoch Step: 100 Loss: 36.973858\n",
            "Epoch Step: 200 Loss: 34.949593\n",
            "Epoch Step: 300 Loss: 39.016918\n",
            "Epoch Step: 400 Loss: 32.437199\n",
            "Epoch Step: 500 Loss: 34.945236\n",
            "Epoch Step: 600 Loss: 38.693138\n",
            "Epoch Step: 700 Loss: 36.923573\n",
            "Epoch Step: 800 Loss: 40.154560\n",
            "Validation perplexity: 17.995239\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAEWCAYAAABhffzLAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nqGbDslQAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VMpL-_MwJOws",
        "colab_type": "text"
      },
      "source": [
        "This is the function used to decode the model output. For simplicity, we use greedy search here."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M1HTqYwy6-yL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Note: you might have to modify this `greedy_decode` function to work with your\n",
        "# `EncoderAttentionDecoder`.\n",
        "\n",
        "def greedy_decode(model, src_ids, src_lengths, max_len):\n",
        "  \"\"\"Greedily decode a sentence for EncoderDecoder.\"\"\"\n",
        "\n",
        "  with torch.no_grad():\n",
        "    encoder_hiddens, encoder_finals = model.encode(src_ids, src_lengths)\n",
        "    prev_y = torch.ones(1, 1).fill_(SOS_INDEX).type_as(src_ids)\n",
        "\n",
        "  output = []\n",
        "  hidden = None\n",
        "  src_mask = torch.where(src_ids == PAD_INDEX, torch.zeros(src_ids.size()).to(device), torch.ones(src_ids.size()).to(device))\n",
        "  src_mask.unsqueeze_(1)\n",
        "\n",
        "  for i in range(max_len):\n",
        "    with torch.no_grad():\n",
        "      hidden, outputs = model.decode(prev_y, encoder_finals, encoder_hiddens, src_mask=src_mask, decoder_hidden=hidden)\n",
        "      prob = model.generator(outputs[:, -1])\n",
        "\n",
        "    _, next_word = torch.max(prob, dim=1)\n",
        "    next_word = next_word.data.item()\n",
        "    output.append(next_word)\n",
        "    prev_y = torch.ones(1, 1).type_as(src_ids).fill_(next_word)\n",
        "\n",
        "  output = np.array(output)\n",
        "\n",
        "  # Cut off everything starting from </s>.\n",
        "  first_eos = np.where(output == EOS_INDEX)[0]\n",
        "  if len(first_eos) > 0:\n",
        "    output = output[:first_eos[0]]\n",
        "  return output\n",
        "  \n",
        "\n",
        "def lookup_words(x, vocab):\n",
        "  return [vocab[i] for i in x]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d_5go3VxJZKh",
        "colab_type": "text"
      },
      "source": [
        "Print the top 3 examples from the data loader by applying the greedy decoder."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cc3m4optFrb3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def print_examples(model, data_loader, n=3,\n",
        "                   max_len=MAX_SENT_LENGTH_PLUS_SOS_EOS, \n",
        "                   src_vocab_set=src_vocab_set, trg_vocab_set=trg_vocab_set):\n",
        "  \"\"\"Prints `n` examples. Assumes batch size of 1.\"\"\"\n",
        "\n",
        "  model.eval()\n",
        "\n",
        "  for i, (src_ids, src_lengths, trg_ids, _) in enumerate(data_loader):\n",
        "    result = greedy_decode(model, src_ids.to(device), src_lengths.to(device),\n",
        "                           max_len=max_len)\n",
        "\n",
        "    # remove <s>\n",
        "    src_ids = src_ids[0, 1:]\n",
        "    trg_ids = trg_ids[0, 1:]\n",
        "    # remove </s> and <pad>\n",
        "    src_ids = src_ids[:np.where(src_ids == EOS_INDEX)[0][0]]\n",
        "    trg_ids = trg_ids[:np.where(trg_ids == EOS_INDEX)[0][0]]\n",
        "\n",
        "    print(\"Example #%d\" % (i + 1))\n",
        "    print(\"Src : \", \" \".join(lookup_words(src_ids, vocab=src_vocab_set)))\n",
        "    print(\"Trg : \", \" \".join(lookup_words(trg_ids, vocab=trg_vocab_set)))\n",
        "    print(\"Pred: \", \" \".join(lookup_words(result, vocab=trg_vocab_set)))\n",
        "    print()\n",
        "\n",
        "    if i == n - 1:\n",
        "      break"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l7Hi4X0GJouK",
        "colab_type": "text"
      },
      "source": [
        "Here we use the validation dataset to print examples."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "27thJIfreCgB",
        "colab_type": "code",
        "outputId": "c7d4346e-f9fe-44dc-a84e-72e525e56e21",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 277
        }
      },
      "source": [
        "example_set = MTDataset(val_src_sentences_list, src_vocab_set,\n",
        "                        val_trg_sentences_list, trg_vocab_set)\n",
        "example_data_loader = data.DataLoader(val_set, batch_size=1, num_workers=1,\n",
        "                                      shuffle=False)\n",
        "\n",
        "print_examples(pure_seq2seq, example_data_loader)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Example #1\n",
            "Src :  Khoa học đằng sau một tiêu đề về khí hậu\n",
            "Trg :  Rachel <unk> : The science behind a climate headline\n",
            "Pred:  The <unk> of the Higgs boson is a bit .\n",
            "\n",
            "Example #2\n",
            "Src :  Tôi muốn cho các bạn biết về sự to lớn của những nỗ lực khoa học đã góp phần làm nên các dòng tít bạn thường thấy trên báo .\n",
            "Trg :  I &apos;d like to talk to you today about the scale of the scientific effort that goes into making the headlines you see in the paper .\n",
            "Pred:  I want to show you what the most exciting thing about the most important lesson in the world is to look at the <unk> of the University of the history .\n",
            "\n",
            "Example #3\n",
            "Src :  Có những dòng trông như thế này khi bàn về biến đổi khí hậu , và như thế này khi nói về chất lượng không khí hay khói bụi .\n",
            "Trg :  <unk> that look like this when they have to do with climate change , and headlines that look like this when they have to do with air quality or smog .\n",
            "Pred:  There &apos;s a lot of anger , like this , the <unk> , the <unk> , or the <unk> , the <unk> , the <unk> .\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H5pTV5PqJtX4",
        "colab_type": "text"
      },
      "source": [
        "Compute the BLEU score. BLEU score is a standard measure to evaluate the translation results. For further details, you can refer to [this](https://en.wikipedia.org/wiki/BLEU) link."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6XGQYwHRPyne",
        "colab_type": "code",
        "outputId": "af74ecf0-3a02-4318-95bb-55895fb0fd8f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        }
      },
      "source": [
        "import sacrebleu\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "def compute_BLEU(model, data_loader):\n",
        "  bleu_score = []\n",
        "\n",
        "  model.eval()\n",
        "  for src_ids, src_lengths, trg_ids, _ in tqdm(data_loader):\n",
        "    result = greedy_decode(model, src_ids.to(device), src_lengths.to(device),\n",
        "                           max_len=MAX_SENT_LENGTH_PLUS_SOS_EOS)\n",
        "    # remove <s>\n",
        "    src_ids = src_ids[0, 1:]\n",
        "    trg_ids = trg_ids[0, 1:]\n",
        "    # remove </s> and <pad>\n",
        "    src_ids = src_ids[:np.where(src_ids == EOS_INDEX)[0][0]]\n",
        "    trg_ids = trg_ids[:np.where(trg_ids == EOS_INDEX)[0][0]]\n",
        "\n",
        "    pred = \" \".join(lookup_words(result, vocab=trg_vocab_set))\n",
        "    targ = \" \".join(lookup_words(trg_ids, vocab=trg_vocab_set))\n",
        "\n",
        "    bleu_score.append(sacrebleu.raw_corpus_bleu([pred], [[targ]], .01).score)\n",
        "\n",
        "  return bleu_score\n",
        "\n",
        "\n",
        "test_set = MTDataset(test_src_sentences_list, src_vocab_set,\n",
        "                     test_trg_sentences_list, trg_vocab_set, sampling=1.)\n",
        "test_data_loader = data.DataLoader(test_set, batch_size=1, num_workers=8,\n",
        "                                   shuffle=False)\n",
        "\n",
        "print('BLEU score:', np.mean(compute_BLEU(pure_seq2seq, test_data_loader)))\n",
        "print('BLEU score:', np.mean(compute_BLEU(attn_seq2seq, test_data_loader)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1139/1139 [00:51<00:00, 22.27it/s]\n",
            "  0%|          | 0/1139 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "BLEU score:  5.699264199362628\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1139/1139 [01:16<00:00, 14.90it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "BLEU score:  14.089084256464574\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GbOgwJw_CkCW",
        "colab_type": "text"
      },
      "source": [
        "## **Part 3: Lab writeup**\n",
        "\n",
        "Your lab report should discuss any implementation details that were important to filling out the code above. Then, use the code to set up experiments that answer the following questions:\n",
        "\n",
        "1. In this lab we use greedy search for decoding, that is, always taking the most probable word at current timestep as prediction. Describe an alternative decoding method that might work better than greedy search. You don't have to implement it.\n",
        "\n",
        "2. Pick some samples from dev or test set and visualize their attention maps. Discuss your findings. Hint: compute the attention scores on the input words for each timestep during decoding.\n",
        "\n",
        "3. Compare the performance of seq2seq with and without attention on sentences of different lengths. You can set some length intervals (e.g., 1-10, 11-20, 21-30, 31-40, 41-50) and compare the two models' performance within each length interval. Discuss your findings.\n",
        "\n",
        "4. Try to improve your BLEU score. For example, try stacking more RNN layers, switching cell types, or applying bi-direction to encoder. Describe what you try, even if they don't show improvement. Hints:\n",
        "  * TA's preliminary implementation of seq2seq with attention model achieves around 16. You don't have to surpass it (although it's pretty simple to do so)--this number is just to give you some sense of what a baseline should get.\n",
        "  * Training on the entire training set takes some time. So tune your hyperparameters on a smaller training set (you can do so by changing `sampling` when creating the data loader)."
      ]
    }
  ]
}